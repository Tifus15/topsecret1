Config file content:
{'model': 'PULL', 'sob': [True, True], 'sob_a': [1.0, 0.15], 'a': 0.01, 'opti': 'adamW', 'reg': 'none', 'loss': 'Huber', 'acts': ['relu', None], 'epochs': 100, 'modelsize': 256, 'batchsize': 128, 'timesize': 32, 'lr': 0.0001, 'split': 0.9, 'device': 'cpu', 'single': False, 'noloops': True, 'samples': 15, 'bias': True}
100
GNN_maker_HNN(
  (net): Sequential(
    (0): PulloutLayer(
      (in_w): Linear(in_features=2, out_features=128, bias=True)
      (out_w): Linear(in_features=2, out_features=128, bias=True)
    )
    (1): Tanh()
    (2): PulloutLayer(
      (in_w): Linear(in_features=128, out_features=6, bias=True)
      (out_w): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
torch.Size([128, 15, 1, 5])
15
torch.Size([32, 1, 4])
GNN_maker_HNN(
  (net): Sequential(
    (0): PulloutLayer(
      (in_w): Linear(in_features=2, out_features=128, bias=True)
      (out_w): Linear(in_features=2, out_features=128, bias=True)
    )
    (1): Tanh()
    (2): PulloutLayer(
      (in_w): Linear(in_features=128, out_features=6, bias=True)
      (out_w): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
TRAIN BATCHES : 10
TEST BATCHES : 1
TRAIN
  0%|                                                                                                             | 0/100 [00:00<?, ?it/s]







 90%|███████████████████████████████████████████████████████████████████████████████████████████▊          | 9/10 [00:14<00:01,  1.65s/it]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:16<00:00,  1.63s/it]
  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]
Epoch: 1
LOSS: train: 2.139508   ham: 10.291967 |   test: 2.037159  ham: 9.452106
{'net.0.in_w.weight_grad': -0.0002295980229973793, 'net.0.in_w.bias_grad': -0.0011878807563334703, 'net.0.out_w.weight_grad': -0.0002295980229973793, 'net.0.out_w.bias_grad': -0.0011878807563334703, 'net.2.in_w.weight_grad': 0.003442700020968914, 'net.2.in_w.bias_grad': -0.14813588559627533, 'net.2.out_w.weight_grad': 0.003442700020968914, 'net.2.out_w.bias_grad': -0.14813588559627533}
  1%|█                                                                                                    | 1/100 [00:18<30:15, 18.34s/it]







 90%|███████████████████████████████████████████████████████████████████████████████████████████▊          | 9/10 [00:14<00:01,  1.66s/it]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:16<00:00,  1.65s/it]
  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]
Epoch: 2
LOSS: train: 2.083219   ham: 9.904124 |   test: 1.993541  ham: 9.128747
{'net.0.in_w.weight_grad': -0.00031897961162030697, 'net.0.in_w.bias_grad': -0.001038139220327139, 'net.0.out_w.weight_grad': -0.00031897961162030697, 'net.0.out_w.bias_grad': -0.001038139220327139, 'net.2.in_w.weight_grad': 0.0037563974037766457, 'net.2.in_w.bias_grad': -0.14335662126541138, 'net.2.out_w.weight_grad': 0.0037563974037766457, 'net.2.out_w.bias_grad': -0.14335662126541138}
  2%|██                                                                                                   | 2/100 [00:36<29:48, 18.25s/it]







100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:16<00:00,  1.66s/it]
  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]
  3%|███                                                                                                  | 3/100 [00:54<29:35, 18.30s/it]
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
Epoch: 3
LOSS: train: 2.032969   ham: 9.539338 |   test: 1.969460  ham: 8.915474
{'net.0.in_w.weight_grad': -0.00022979089408181608, 'net.0.in_w.bias_grad': -0.0008572973310947418, 'net.0.out_w.weight_grad': -0.00022979089408181608, 'net.0.out_w.bias_grad': -0.0008572973310947418, 'net.2.in_w.weight_grad': 0.003590049920603633, 'net.2.in_w.bias_grad': -0.13597524166107178, 'net.2.out_w.weight_grad': 0.003590049920603633, 'net.2.out_w.bias_grad': -0.13597524166107178}







100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:16<00:00,  1.68s/it]
  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]
  4%|████                                                                                                 | 4/100 [01:13<29:17, 18.31s/it]
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
Epoch: 4
LOSS: train: 1.986735   ham: 9.197205 |   test: 1.889029  ham: 8.417550
{'net.0.in_w.weight_grad': -0.00021339433442335576, 'net.0.in_w.bias_grad': -0.0007897935574874282, 'net.0.out_w.weight_grad': -0.00021339433442335576, 'net.0.out_w.bias_grad': -0.0007897935574874282, 'net.2.in_w.weight_grad': 0.003360152244567871, 'net.2.in_w.bias_grad': -0.1263662725687027, 'net.2.out_w.weight_grad': 0.003360152244567871, 'net.2.out_w.bias_grad': -0.1263662725687027}







100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:16<00:00,  1.64s/it]
  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]
  5%|█████                                                                                                | 5/100 [01:31<28:53, 18.25s/it]
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
Epoch: 5
LOSS: train: 1.957160   ham: 8.972702 |   test: 1.915228  ham: 8.475055
{'net.0.in_w.weight_grad': 0.00021299783838912845, 'net.0.in_w.bias_grad': -0.000574500416405499, 'net.0.out_w.weight_grad': 0.00021299783838912845, 'net.0.out_w.bias_grad': -0.000574500416405499, 'net.2.in_w.weight_grad': 0.000662767852190882, 'net.2.in_w.bias_grad': -0.11083414405584335, 'net.2.out_w.weight_grad': 0.000662767852190882, 'net.2.out_w.bias_grad': -0.11083414405584335}







100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:16<00:00,  1.64s/it]
  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]
  6%|██████                                                                                               | 6/100 [01:49<28:31, 18.21s/it]
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
Epoch: 6
LOSS: train: 1.933725   ham: 8.794683 |   test: 1.910601  ham: 8.262281
{'net.0.in_w.weight_grad': 0.0002587990020401776, 'net.0.in_w.bias_grad': -0.0004021654021926224, 'net.0.out_w.weight_grad': 0.0002587990020401776, 'net.0.out_w.bias_grad': -0.0004021654021926224, 'net.2.in_w.weight_grad': 3.983072474511573e-06, 'net.2.in_w.bias_grad': -0.09635823220014572, 'net.2.out_w.weight_grad': 3.983072474511573e-06, 'net.2.out_w.bias_grad': -0.09635823220014572}







100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:16<00:00,  1.66s/it]
  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]
  7%|███████                                                                                              | 7/100 [02:07<28:08, 18.16s/it]
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
Epoch: 7
LOSS: train: 1.914250   ham: 8.644709 |   test: 1.943441  ham: 8.460427
{'net.0.in_w.weight_grad': 0.00018759439990390092, 'net.0.in_w.bias_grad': -0.00041377084562554955, 'net.0.out_w.weight_grad': 0.00018759439990390092, 'net.0.out_w.bias_grad': -0.00041377084562554955, 'net.2.in_w.weight_grad': 0.0012362952111288905, 'net.2.in_w.bias_grad': -0.1076897606253624, 'net.2.out_w.weight_grad': 0.0012362952111288905, 'net.2.out_w.bias_grad': -0.1076897606253624}







100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:16<00:00,  1.65s/it]
  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]
  8%|████████                                                                                             | 8/100 [02:25<27:47, 18.12s/it]
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
Epoch: 8
LOSS: train: 1.898165   ham: 8.523781 |   test: 1.882180  ham: 8.137883
{'net.0.in_w.weight_grad': -3.798153920797631e-05, 'net.0.in_w.bias_grad': -0.00027705245884135365, 'net.0.out_w.weight_grad': -3.798153920797631e-05, 'net.0.out_w.bias_grad': -0.00027705245884135365, 'net.2.in_w.weight_grad': 0.002437399700284004, 'net.2.in_w.bias_grad': -0.0962384045124054, 'net.2.out_w.weight_grad': 0.002437399700284004, 'net.2.out_w.bias_grad': -0.0962384045124054}








 90%|███████████████████████████████████████████████████████████████████████████████████████████▊          | 9/10 [00:15<00:01,  1.67s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.51s/it]
  9%|█████████                                                                                            | 9/100 [02:43<27:33, 18.18s/it]
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
Epoch: 9
LOSS: train: 1.884014   ham: 8.421671 |   test: 1.779532  ham: 7.606422
{'net.0.in_w.weight_grad': 0.0001814151182770729, 'net.0.in_w.bias_grad': -0.0001725829642964527, 'net.0.out_w.weight_grad': 0.0001814151182770729, 'net.0.out_w.bias_grad': -0.0001725829642964527, 'net.2.in_w.weight_grad': 0.0009386723977513611, 'net.2.in_w.bias_grad': -0.08237367123365402, 'net.2.out_w.weight_grad': 0.0009386723977513611, 'net.2.out_w.bias_grad': -0.08237367123365402}








 90%|███████████████████████████████████████████████████████████████████████████████████████████▊          | 9/10 [00:14<00:01,  1.64s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.50s/it]
 10%|██████████                                                                                          | 10/100 [03:01<27:12, 18.14s/it]
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
Epoch: 10
LOSS: train: 1.872558   ham: 8.339734 |   test: 1.830240  ham: 7.804462
{'net.0.in_w.weight_grad': 0.00019243391579948366, 'net.0.in_w.bias_grad': -0.0002317311882507056, 'net.0.out_w.weight_grad': 0.00019243391579948366, 'net.0.out_w.bias_grad': -0.0002317311882507056, 'net.2.in_w.weight_grad': 0.0006449743523262441, 'net.2.in_w.bias_grad': -0.08641860634088516, 'net.2.out_w.weight_grad': 0.0006449743523262441, 'net.2.out_w.bias_grad': -0.08641860634088516}







100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:16<00:00,  1.63s/it]
  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]
 11%|███████████                                                                                         | 11/100 [03:19<26:48, 18.07s/it]
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
Epoch: 11
LOSS: train: 1.859982   ham: 8.260668 |   test: 1.828594  ham: 7.738246
{'net.0.in_w.weight_grad': 0.00031175700132735074, 'net.0.in_w.bias_grad': -1.750375668052584e-05, 'net.0.out_w.weight_grad': 0.00031175700132735074, 'net.0.out_w.bias_grad': -1.750375668052584e-05, 'net.2.in_w.weight_grad': -0.00014144238957669586, 'net.2.in_w.bias_grad': -0.060079220682382584, 'net.2.out_w.weight_grad': -0.00014144238957669586, 'net.2.out_w.bias_grad': -0.060079220682382584}








 90%|███████████████████████████████████████████████████████████████████████████████████████████▊          | 9/10 [00:14<00:01,  1.65s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.49s/it]
 12%|████████████                                                                                        | 12/100 [03:37<26:29, 18.06s/it]
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
Epoch: 12
LOSS: train: 1.847330   ham: 8.190505 |   test: 1.797679  ham: 7.639735
{'net.0.in_w.weight_grad': 0.00028885717620141804, 'net.0.in_w.bias_grad': -0.00012981996405869722, 'net.0.out_w.weight_grad': 0.00028885717620141804, 'net.0.out_w.bias_grad': -0.00012981996405869722, 'net.2.in_w.weight_grad': 2.0599885829142295e-05, 'net.2.in_w.bias_grad': -0.07818170636892319, 'net.2.out_w.weight_grad': 2.0599885829142295e-05, 'net.2.out_w.bias_grad': -0.07818170636892319}







100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:16<00:00,  1.63s/it]
  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]
 13%|█████████████                                                                                       | 13/100 [03:55<26:09, 18.04s/it]
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
Epoch: 13
LOSS: train: 1.838473   ham: 8.133964 |   test: 1.798782  ham: 7.559429
{'net.0.in_w.weight_grad': 0.00022869589156471193, 'net.0.in_w.bias_grad': -6.738575757481158e-05, 'net.0.out_w.weight_grad': 0.00022869589156471193, 'net.0.out_w.bias_grad': -6.738575757481158e-05, 'net.2.in_w.weight_grad': 0.001093279104679823, 'net.2.in_w.bias_grad': -0.07361803203821182, 'net.2.out_w.weight_grad': 0.001093279104679823, 'net.2.out_w.bias_grad': -0.07361803203821182}








 90%|███████████████████████████████████████████████████████████████████████████████████████████▊          | 9/10 [00:14<00:01,  1.66s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.59s/it]
 14%|██████████████                                                                                      | 14/100 [04:13<25:53, 18.06s/it]
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
Epoch: 14
LOSS: train: 1.826059   ham: 8.067225 |   test: 1.769705  ham: 7.415099
{'net.0.in_w.weight_grad': 0.0002826681302394718, 'net.0.in_w.bias_grad': -6.1019964050501585e-05, 'net.0.out_w.weight_grad': 0.0002826681302394718, 'net.0.out_w.bias_grad': -6.1019964050501585e-05, 'net.2.in_w.weight_grad': 0.0006329328170977533, 'net.2.in_w.bias_grad': -0.06444033980369568, 'net.2.out_w.weight_grad': 0.0006329328170977533, 'net.2.out_w.bias_grad': -0.06444033980369568}








 90%|███████████████████████████████████████████████████████████████████████████████████████████▊          | 9/10 [00:14<00:01,  1.65s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.54s/it]
 15%|███████████████                                                                                     | 15/100 [04:32<25:39, 18.11s/it]
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
Epoch: 15
LOSS: train: 1.811639   ham: 7.985873 |   test: 1.744299  ham: 7.325661
{'net.0.in_w.weight_grad': 5.654005872202106e-05, 'net.0.in_w.bias_grad': -9.945686906576157e-05, 'net.0.out_w.weight_grad': 5.654005872202106e-05, 'net.0.out_w.bias_grad': -9.945686906576157e-05, 'net.2.in_w.weight_grad': 0.0032268157228827477, 'net.2.in_w.bias_grad': -0.09226468205451965, 'net.2.out_w.weight_grad': 0.0032268157228827477, 'net.2.out_w.bias_grad': -0.09226468205451965}








 90%|███████████████████████████████████████████████████████████████████████████████████████████▊          | 9/10 [00:14<00:01,  1.67s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.60s/it]
 16%|████████████████                                                                                    | 16/100 [04:50<25:20, 18.10s/it]
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
Epoch: 16
LOSS: train: 1.800964   ham: 7.928618 |   test: 1.800839  ham: 7.578436
{'net.0.in_w.weight_grad': 0.0002788116689771414, 'net.0.in_w.bias_grad': -6.766528531443328e-05, 'net.0.out_w.weight_grad': 0.0002788116689771414, 'net.0.out_w.bias_grad': -6.766528531443328e-05, 'net.2.in_w.weight_grad': 0.0005261078476905823, 'net.2.in_w.bias_grad': -0.07507514208555222, 'net.2.out_w.weight_grad': 0.0005261078476905823, 'net.2.out_w.bias_grad': -0.07507514208555222}








 90%|███████████████████████████████████████████████████████████████████████████████████████████▊          | 9/10 [00:14<00:01,  1.65s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.58s/it]
 17%|█████████████████                                                                                   | 17/100 [05:08<25:01, 18.09s/it]
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
Epoch: 17
LOSS: train: 1.787192   ham: 7.854608 |   test: 1.727227  ham: 7.256004
{'net.0.in_w.weight_grad': 0.00035059283254668117, 'net.0.in_w.bias_grad': 9.810902702156454e-05, 'net.0.out_w.weight_grad': 0.00035059283254668117, 'net.0.out_w.bias_grad': 9.810902702156454e-05, 'net.2.in_w.weight_grad': -0.0006535709835588932, 'net.2.in_w.bias_grad': -0.0642726942896843, 'net.2.out_w.weight_grad': -0.0006535709835588932, 'net.2.out_w.bias_grad': -0.0642726942896843}








 90%|███████████████████████████████████████████████████████████████████████████████████████████▊          | 9/10 [00:14<00:01,  1.65s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.57s/it]
 18%|██████████████████                                                                                  | 18/100 [05:26<24:41, 18.06s/it]
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
Epoch: 18
LOSS: train: 1.776544   ham: 7.791700 |   test: 1.733958  ham: 7.363333
{'net.0.in_w.weight_grad': 0.0003099826571997255, 'net.0.in_w.bias_grad': 2.424539707135409e-05, 'net.0.out_w.weight_grad': 0.0003099826571997255, 'net.0.out_w.bias_grad': 2.424539707135409e-05, 'net.2.in_w.weight_grad': 0.0015402153367176652, 'net.2.in_w.bias_grad': -0.07244046777486801, 'net.2.out_w.weight_grad': 0.0015402153367176652, 'net.2.out_w.bias_grad': -0.07244046777486801}








 90%|███████████████████████████████████████████████████████████████████████████████████████████▊          | 9/10 [00:14<00:01,  1.65s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.56s/it]
 19%|███████████████████                                                                                 | 19/100 [05:44<24:20, 18.03s/it]
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
Epoch: 19
LOSS: train: 1.760658   ham: 7.701386 |   test: 1.712454  ham: 7.134327
{'net.0.in_w.weight_grad': 0.000248343450948596, 'net.0.in_w.bias_grad': 0.00012092781980754808, 'net.0.out_w.weight_grad': 0.000248343450948596, 'net.0.out_w.bias_grad': 0.00012092781980754808, 'net.2.in_w.weight_grad': 0.000629547459539026, 'net.2.in_w.bias_grad': -0.054444570094347, 'net.2.out_w.weight_grad': 0.000629547459539026, 'net.2.out_w.bias_grad': -0.054444570094347}








100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:16<00:00,  1.67s/it]
  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]
TEST
Epoch: 20
LOSS: train: 1.749627   ham: 7.641567 |   test: 1.675706  ham: 7.007523
{'net.0.in_w.weight_grad': 0.0003236043849028647, 'net.0.in_w.bias_grad': -4.0543804061599076e-05, 'net.0.out_w.weight_grad': 0.0003236043849028647, 'net.0.out_w.bias_grad': -4.0543804061599076e-05, 'net.2.in_w.weight_grad': 0.0020385764073580503, 'net.2.in_w.bias_grad': -0.09252816438674927, 'net.2.out_w.weight_grad': 0.0020385764073580503, 'net.2.out_w.bias_grad': -0.09252816438674927}
 20%|████████████████████                                                                                | 20/100 [06:02<24:07, 18.09s/it]







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.48s/it]
 21%|█████████████████████                                                                               | 21/100 [06:20<23:45, 18.05s/it]
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
TEST
Epoch: 21
LOSS: train: 1.737529   ham: 7.571267 |   test: 1.704880  ham: 7.151782
{'net.0.in_w.weight_grad': 0.00029604946030303836, 'net.0.in_w.bias_grad': 8.918861567508429e-05, 'net.0.out_w.weight_grad': 0.00029604946030303836, 'net.0.out_w.bias_grad': 8.918861567508429e-05, 'net.2.in_w.weight_grad': 0.00182126194704324, 'net.2.in_w.bias_grad': -0.06522051244974136, 'net.2.out_w.weight_grad': 0.00182126194704324, 'net.2.out_w.bias_grad': -0.06522051244974136}








 90%|███████████████████████████████████████████████████████████████████████████████████████████▊          | 9/10 [00:14<00:01,  1.63s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.62s/it]
 22%|██████████████████████                                                                              | 22/100 [06:38<23:27, 18.05s/it]
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
Epoch: 22
LOSS: train: 1.723921   ham: 7.501630 |   test: 1.685779  ham: 7.022191
{'net.0.in_w.weight_grad': 0.0003201704821549356, 'net.0.in_w.bias_grad': 2.2909371182322502e-06, 'net.0.out_w.weight_grad': 0.0003201704821549356, 'net.0.out_w.bias_grad': 2.2909371182322502e-06, 'net.2.in_w.weight_grad': 0.002035606186836958, 'net.2.in_w.bias_grad': -0.07669952511787415, 'net.2.out_w.weight_grad': 0.002035606186836958, 'net.2.out_w.bias_grad': -0.07669952511787415}








100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:16<00:00,  1.66s/it]
  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]
TEST
Epoch: 23
LOSS: train: 1.712023   ham: 7.431192 |   test: 1.631480  ham: 6.633594
{'net.0.in_w.weight_grad': 0.00040582625661045313, 'net.0.in_w.bias_grad': 8.793531742412597e-05, 'net.0.out_w.weight_grad': 0.00040582625661045313, 'net.0.out_w.bias_grad': 8.793531742412597e-05, 'net.2.in_w.weight_grad': 0.0016238657990470529, 'net.2.in_w.bias_grad': -0.06741278618574142, 'net.2.out_w.weight_grad': 0.0016238657990470529, 'net.2.out_w.bias_grad': -0.06741278618574142}
 23%|███████████████████████                                                                             | 23/100 [06:56<23:15, 18.12s/it]







100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:16<00:00,  1.64s/it]
  0%|                                                                                                               | 0/1 [00:00<?, ?it/s]
TEST
Epoch: 24
LOSS: train: 1.694714   ham: 7.342238 |   test: 1.668409  ham: 6.901776
{'net.0.in_w.weight_grad': 0.0003519177553243935, 'net.0.in_w.bias_grad': 7.155277126003057e-05, 'net.0.out_w.weight_grad': 0.0003519177553243935, 'net.0.out_w.bias_grad': 7.155277126003057e-05, 'net.2.in_w.weight_grad': 0.0004378494340926409, 'net.2.in_w.bias_grad': -0.06713422387838364, 'net.2.out_w.weight_grad': 0.0004378494340926409, 'net.2.out_w.bias_grad': -0.06713422387838364}
  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]
 24%|████████████████████████                                                                            | 24/100 [07:15<22:59, 18.14s/it]
Traceback (most recent call last):
  File "/home/ias/Desktop/theend/topsecret1/MAIN/pend_free/main_pend.py", line 994, in <module>
    train1dof(configs)
  File "/home/ias/Desktop/theend/topsecret1/MAIN/pend_free/main_pend.py", line 209, in train1dof
    x_pred = Euler_for_learning(model,x0,ts)
  File "/home/ias/Desktop/theend/topsecret1/MAIN/pend_free/utils.py", line 182, in Euler_for_learning
    K1 = evaluate_model(model,out_l[i-1].squeeze())
  File "/home/ias/Desktop/theend/topsecret1/MAIN/pend_free/utils.py", line 171, in evaluate_model
    h_pred = model(x)
  File "/home/ias/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ias/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1561, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/ias/Desktop/theend/topsecret1/MAIN/pend_free/HGNN.py", line 211, in forward
    gs = dgl.unbatch(self.g_hnn)
  File "/home/ias/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/dgl/batch.py", line 424, in unbatch
    gs = [
  File "/home/ias/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/dgl/batch.py", line 425, in <listcomp>
    convert.heterograph(edge_dict, num_nodes_dict, idtype=g.idtype)
  File "/home/ias/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/dgl/convert.py", line 331, in heterograph
    (sparse_fmt, arrays), urange, vrange = utils.graphdata2tensors(
  File "/home/ias/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/dgl/utils/data.py", line 194, in graphdata2tensors
    num_src, num_dst = infer_num_nodes(data, bipartite=bipartite)
  File "/home/ias/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/dgl/utils/data.py", line 339, in infer_num_nodes
    ndst = F.as_scalar(F.max(v, dim=0)) + 1 if len(v) > 0 else 0
  File "/home/ias/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py", line 178, in max
    return th.max(input, dim=dim)[0]
KeyboardInterrupt