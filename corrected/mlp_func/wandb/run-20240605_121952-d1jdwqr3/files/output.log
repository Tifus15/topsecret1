
  1%|▋                                                                                                                          | 3/500 [00:01<03:53,  2.13it/s]
Epoch: 1
LOSS: train: 0.225540 grad:0.004170   |   test: 0.202688 grad: 0.003777
Epoch: 2
LOSS: train: 0.204731 grad:0.003889   |   test: 0.184763 grad: 0.003535
Epoch: 3
LOSS: train: 0.185757 grad:0.003652   |   test: 0.168532 grad: 0.003322
Epoch: 4

  2%|██▏                                                                                                                        | 9/500 [00:03<03:04,  2.66it/s]
Epoch: 5
LOSS: train: 0.153725 grad:0.003235   |   test: 0.141684 grad: 0.002972
Epoch: 6
LOSS: train: 0.140683 grad:0.003074   |   test: 0.130275 grad: 0.002824
Epoch: 7
LOSS: train: 0.129171 grad:0.002935   |   test: 0.120032 grad: 0.002689
Epoch: 8
LOSS: train: 0.118542 grad:0.002794   |   test: 0.110895 grad: 0.002568
Epoch: 9
LOSS: train: 0.108712 grad:0.002653   |   test: 0.102700 grad: 0.002458
Epoch: 10

  3%|███▏                                                                                                                      | 13/500 [00:05<03:20,  2.43it/s]
Epoch: 11
LOSS: train: 0.093593 grad:0.002455   |   test: 0.088486 grad: 0.002262
Epoch: 12
LOSS: train: 0.086623 grad:0.002358   |   test: 0.082276 grad: 0.002174
Epoch: 13
LOSS: train: 0.080264 grad:0.002273   |   test: 0.076564 grad: 0.002092
Epoch: 14

  4%|████▍                                                                                                                     | 18/500 [00:07<03:17,  2.44it/s]
Epoch: 15
LOSS: train: 0.069772 grad:0.002115   |   test: 0.066374 grad: 0.001942
Epoch: 16
LOSS: train: 0.064636 grad:0.002044   |   test: 0.061761 grad: 0.001872
Epoch: 17
LOSS: train: 0.060384 grad:0.001978   |   test: 0.057500 grad: 0.001806
Epoch: 18
LOSS: train: 0.056238 grad:0.001909   |   test: 0.053547 grad: 0.001743
Epoch: 19

  5%|█████▌                                                                                                                    | 23/500 [00:09<03:32,  2.24it/s]
Epoch: 20
LOSS: train: 0.049536 grad:0.001800   |   test: 0.046309 grad: 0.001623
Epoch: 21
LOSS: train: 0.045575 grad:0.001725   |   test: 0.043010 grad: 0.001567
Epoch: 22
LOSS: train: 0.043157 grad:0.001688   |   test: 0.039938 grad: 0.001514
Epoch: 23
LOSS: train: 0.040353 grad:0.001634   |   test: 0.037149 grad: 0.001464
Epoch: 24

  6%|██████▊                                                                                                                   | 28/500 [00:11<03:12,  2.45it/s]
Epoch: 25
LOSS: train: 0.035508 grad:0.001547   |   test: 0.032175 grad: 0.001373
Epoch: 26
LOSS: train: 0.033229 grad:0.001498   |   test: 0.030015 grad: 0.001332
Epoch: 27
LOSS: train: 0.031161 grad:0.001461   |   test: 0.028007 grad: 0.001293
Epoch: 28
LOSS: train: 0.029309 grad:0.001416   |   test: 0.026227 grad: 0.001258
Epoch: 29

  7%|████████                                                                                                                  | 33/500 [00:13<03:05,  2.51it/s]
Epoch: 30
LOSS: train: 0.025995 grad:0.001342   |   test: 0.023191 grad: 0.001196
Epoch: 31
LOSS: train: 0.025078 grad:0.001326   |   test: 0.021893 grad: 0.001168
Epoch: 32
LOSS: train: 0.023314 grad:0.001284   |   test: 0.020724 grad: 0.001143
Epoch: 33
LOSS: train: 0.022226 grad:0.001262   |   test: 0.019678 grad: 0.001120
Epoch: 34

  8%|█████████▎                                                                                                                | 38/500 [00:15<03:03,  2.52it/s]
Epoch: 35
LOSS: train: 0.020380 grad:0.001215   |   test: 0.017937 grad: 0.001080
Epoch: 36
LOSS: train: 0.019195 grad:0.001181   |   test: 0.017191 grad: 0.001063
Epoch: 37
LOSS: train: 0.018548 grad:0.001165   |   test: 0.016507 grad: 0.001046
Epoch: 38
LOSS: train: 0.017576 grad:0.001143   |   test: 0.015876 grad: 0.001031
Epoch: 39

  8%|██████████▏                                                                                                               | 42/500 [00:17<03:06,  2.46it/s]
Epoch: 40
LOSS: train: 0.016470 grad:0.001116   |   test: 0.014732 grad: 0.001002
Epoch: 41
LOSS: train: 0.015963 grad:0.001099   |   test: 0.014244 grad: 0.000989
Epoch: 42
LOSS: train: 0.015439 grad:0.001085   |   test: 0.013801 grad: 0.000978
Epoch: 43

  9%|███████████▏                                                                                                              | 46/500 [00:19<03:30,  2.16it/s]
Epoch: 44
LOSS: train: 0.014276 grad:0.001051   |   test: 0.013019 grad: 0.000956
Epoch: 45
LOSS: train: 0.013931 grad:0.001042   |   test: 0.012666 grad: 0.000946
Epoch: 46
LOSS: train: 0.013507 grad:0.001026   |   test: 0.012349 grad: 0.000936
Epoch: 47

 10%|████████████▍                                                                                                             | 51/500 [00:21<03:04,  2.43it/s]
Epoch: 48
LOSS: train: 0.012546 grad:0.000995   |   test: 0.011757 grad: 0.000919
Epoch: 49
LOSS: train: 0.012307 grad:0.000990   |   test: 0.011489 grad: 0.000911
Epoch: 50
LOSS: train: 0.011903 grad:0.000976   |   test: 0.011236 grad: 0.000903
Epoch: 51
LOSS: train: 0.011554 grad:0.000966   |   test: 0.010981 grad: 0.000895
Epoch: 52
LOSS: train: 0.011356 grad:0.000962   |   test: 0.010752 grad: 0.000887
Epoch: 53
LOSS: train: 0.011195 grad:0.000952   |   test: 0.010539 grad: 0.000880
Epoch: 54

 11%|█████████████▋                                                                                                            | 56/500 [00:23<03:11,  2.31it/s]
Epoch: 55
LOSS: train: 0.010647 grad:0.000934   |   test: 0.010140 grad: 0.000867
Epoch: 56
LOSS: train: 0.010363 grad:0.000927   |   test: 0.009955 grad: 0.000860
Epoch: 57
LOSS: train: 0.010130 grad:0.000913   |   test: 0.009786 grad: 0.000854
Epoch: 58

 12%|██████████████▉                                                                                                           | 61/500 [00:25<03:19,  2.20it/s]
Epoch: 59
LOSS: train: 0.009796 grad:0.000902   |   test: 0.009463 grad: 0.000843
Epoch: 60
LOSS: train: 0.009656 grad:0.000897   |   test: 0.009325 grad: 0.000837
Epoch: 61
LOSS: train: 0.009320 grad:0.000886   |   test: 0.009183 grad: 0.000832
Epoch: 62
LOSS: train: 0.009277 grad:0.000880   |   test: 0.009060 grad: 0.000827
Epoch: 63

 13%|███████████████▊                                                                                                          | 65/500 [00:27<03:03,  2.37it/s]
Epoch: 64
LOSS: train: 0.009038 grad:0.000875   |   test: 0.008825 grad: 0.000817
Epoch: 65
LOSS: train: 0.008746 grad:0.000862   |   test: 0.008723 grad: 0.000812
Epoch: 66
LOSS: train: 0.008630 grad:0.000856   |   test: 0.008630 grad: 0.000807
Epoch: 67

 14%|█████████████████                                                                                                         | 70/500 [00:29<03:04,  2.33it/s]
Epoch: 68
LOSS: train: 0.008436 grad:0.000849   |   test: 0.008418 grad: 0.000798
Epoch: 69
LOSS: train: 0.008286 grad:0.000842   |   test: 0.008332 grad: 0.000793
Epoch: 70
LOSS: train: 0.008120 grad:0.000833   |   test: 0.008234 grad: 0.000789
Epoch: 71
LOSS: train: 0.008099 grad:0.000834   |   test: 0.008139 grad: 0.000785
Epoch: 72

 15%|██████████████████                                                                                                        | 74/500 [00:31<03:11,  2.23it/s]
Epoch: 73
LOSS: train: 0.007916 grad:0.000830   |   test: 0.007970 grad: 0.000776
Epoch: 74
LOSS: train: 0.007866 grad:0.000825   |   test: 0.007895 grad: 0.000772
Epoch: 75
LOSS: train: 0.007699 grad:0.000815   |   test: 0.007814 grad: 0.000768
Epoch: 76

 16%|███████████████████▎                                                                                                      | 79/500 [00:33<02:58,  2.35it/s]
Epoch: 77
LOSS: train: 0.007554 grad:0.000807   |   test: 0.007672 grad: 0.000761
Epoch: 78
LOSS: train: 0.007390 grad:0.000799   |   test: 0.007597 grad: 0.000757
Epoch: 79
LOSS: train: 0.007397 grad:0.000799   |   test: 0.007531 grad: 0.000753
Epoch: 80
LOSS: train: 0.007354 grad:0.000798   |   test: 0.007459 grad: 0.000749
Epoch: 81

 17%|████████████████████▍                                                                                                     | 84/500 [00:35<03:05,  2.24it/s]
Epoch: 82
LOSS: train: 0.007120 grad:0.000784   |   test: 0.007323 grad: 0.000742
Epoch: 83
LOSS: train: 0.007092 grad:0.000784   |   test: 0.007267 grad: 0.000738
Epoch: 84
LOSS: train: 0.007067 grad:0.000782   |   test: 0.007226 grad: 0.000735
Epoch: 85
LOSS: train: 0.006962 grad:0.000773   |   test: 0.007185 grad: 0.000732
Epoch: 86

 18%|█████████████████████▍                                                                                                    | 88/500 [00:37<02:58,  2.31it/s]
Epoch: 87
LOSS: train: 0.006848 grad:0.000770   |   test: 0.007077 grad: 0.000725
Epoch: 88
LOSS: train: 0.006830 grad:0.000768   |   test: 0.007020 grad: 0.000722
Epoch: 89
LOSS: train: 0.006703 grad:0.000762   |   test: 0.006955 grad: 0.000718
Epoch: 90

 19%|██████████████████████▋                                                                                                   | 93/500 [00:39<02:59,  2.27it/s]
Epoch: 91
LOSS: train: 0.006634 grad:0.000755   |   test: 0.006851 grad: 0.000712
Epoch: 92
LOSS: train: 0.006590 grad:0.000754   |   test: 0.006797 grad: 0.000709
Epoch: 93
LOSS: train: 0.006556 grad:0.000752   |   test: 0.006740 grad: 0.000706
Epoch: 94
LOSS: train: 0.006460 grad:0.000746   |   test: 0.006685 grad: 0.000702
Epoch: 95

 19%|███████████████████████▋                                                                                                  | 97/500 [00:41<03:01,  2.22it/s]
Epoch: 96
LOSS: train: 0.006351 grad:0.000741   |   test: 0.006593 grad: 0.000696
Epoch: 97
LOSS: train: 0.006336 grad:0.000741   |   test: 0.006553 grad: 0.000693
Epoch: 98
LOSS: train: 0.006273 grad:0.000735   |   test: 0.006518 grad: 0.000691
Epoch: 99

 20%|████████████████████████▋                                                                                                | 102/500 [00:43<02:53,  2.30it/s]
Epoch: 100
LOSS: train: 0.006206 grad:0.000729   |   test: 0.006459 grad: 0.000686
Epoch: 101
LOSS: train: 0.006198 grad:0.000730   |   test: 0.006417 grad: 0.000683
Epoch: 102
LOSS: train: 0.006127 grad:0.000726   |   test: 0.006378 grad: 0.000680
Epoch: 103
LOSS: train: 0.006096 grad:0.000723   |   test: 0.006338 grad: 0.000677
Epoch: 104

 21%|█████████████████████████▉                                                                                               | 107/500 [00:45<02:47,  2.35it/s]
Epoch: 105
LOSS: train: 0.005999 grad:0.000717   |   test: 0.006261 grad: 0.000672
Epoch: 106
LOSS: train: 0.005962 grad:0.000714   |   test: 0.006233 grad: 0.000670
Epoch: 107
LOSS: train: 0.005934 grad:0.000715   |   test: 0.006190 grad: 0.000667
Epoch: 108
LOSS: train: 0.005907 grad:0.000708   |   test: 0.006156 grad: 0.000664
Epoch: 109

 22%|███████████████████████████                                                                                              | 112/500 [00:47<02:33,  2.53it/s]
Epoch: 110
LOSS: train: 0.005844 grad:0.000707   |   test: 0.006082 grad: 0.000659
Epoch: 111
LOSS: train: 0.005801 grad:0.000701   |   test: 0.006043 grad: 0.000657
Epoch: 112
LOSS: train: 0.005771 grad:0.000701   |   test: 0.006012 grad: 0.000654
Epoch: 113
LOSS: train: 0.005726 grad:0.000697   |   test: 0.005976 grad: 0.000652
Epoch: 114

 23%|████████████████████████████▎                                                                                            | 117/500 [00:49<02:33,  2.49it/s]
Epoch: 115
LOSS: train: 0.005663 grad:0.000693   |   test: 0.005917 grad: 0.000647
Epoch: 116
LOSS: train: 0.005661 grad:0.000695   |   test: 0.005887 grad: 0.000645
Epoch: 117
LOSS: train: 0.005591 grad:0.000688   |   test: 0.005858 grad: 0.000642
Epoch: 118
LOSS: train: 0.005616 grad:0.000692   |   test: 0.005819 grad: 0.000640
Epoch: 119

 24%|█████████████████████████████▌                                                                                           | 122/500 [00:51<02:27,  2.56it/s]
Epoch: 120
LOSS: train: 0.005487 grad:0.000677   |   test: 0.005760 grad: 0.000635
Epoch: 121
LOSS: train: 0.005518 grad:0.000683   |   test: 0.005731 grad: 0.000633
Epoch: 122
LOSS: train: 0.005453 grad:0.000679   |   test: 0.005709 grad: 0.000631
Epoch: 123
LOSS: train: 0.005414 grad:0.000673   |   test: 0.005691 grad: 0.000629
Epoch: 124

 25%|██████████████████████████████▋                                                                                          | 127/500 [00:53<02:25,  2.56it/s]
Epoch: 125
LOSS: train: 0.005348 grad:0.000670   |   test: 0.005635 grad: 0.000624
Epoch: 126
LOSS: train: 0.005351 grad:0.000671   |   test: 0.005603 grad: 0.000622
Epoch: 127
LOSS: train: 0.005380 grad:0.000673   |   test: 0.005572 grad: 0.000620
Epoch: 128
LOSS: train: 0.005302 grad:0.000666   |   test: 0.005542 grad: 0.000618
Epoch: 129
LOSS: train: 0.005253 grad:0.000662   |   test: 0.005510 grad: 0.000616
Epoch: 130

 27%|████████████████████████████████▏                                                                                        | 133/500 [00:55<02:16,  2.70it/s]
Epoch: 131
LOSS: train: 0.005214 grad:0.000659   |   test: 0.005441 grad: 0.000611
Epoch: 132
LOSS: train: 0.005196 grad:0.000656   |   test: 0.005405 grad: 0.000609
Epoch: 133
LOSS: train: 0.005184 grad:0.000660   |   test: 0.005378 grad: 0.000607
Epoch: 134
LOSS: train: 0.005158 grad:0.000653   |   test: 0.005358 grad: 0.000605
Epoch: 135

 28%|█████████████████████████████████▍                                                                                       | 138/500 [00:57<02:17,  2.64it/s]
Epoch: 136
LOSS: train: 0.005084 grad:0.000649   |   test: 0.005321 grad: 0.000601
Epoch: 137
LOSS: train: 0.005099 grad:0.000651   |   test: 0.005304 grad: 0.000599
Epoch: 138
LOSS: train: 0.005035 grad:0.000646   |   test: 0.005285 grad: 0.000597
Epoch: 139
LOSS: train: 0.005015 grad:0.000643   |   test: 0.005269 grad: 0.000595
Epoch: 140

 29%|██████████████████████████████████▌                                                                                      | 143/500 [00:59<02:24,  2.48it/s]
Epoch: 141
LOSS: train: 0.004998 grad:0.000640   |   test: 0.005236 grad: 0.000592
Epoch: 142
LOSS: train: 0.004987 grad:0.000642   |   test: 0.005215 grad: 0.000590
Epoch: 143
LOSS: train: 0.004948 grad:0.000638   |   test: 0.005187 grad: 0.000588
Epoch: 144
LOSS: train: 0.004922 grad:0.000636   |   test: 0.005152 grad: 0.000586
Epoch: 145

 30%|███████████████████████████████████▊                                                                                     | 148/500 [01:01<02:18,  2.54it/s]
Epoch: 146
LOSS: train: 0.004880 grad:0.000633   |   test: 0.005095 grad: 0.000582
Epoch: 147
LOSS: train: 0.004891 grad:0.000636   |   test: 0.005062 grad: 0.000580
Epoch: 148
LOSS: train: 0.004827 grad:0.000626   |   test: 0.005038 grad: 0.000578
Epoch: 149
LOSS: train: 0.004808 grad:0.000623   |   test: 0.005016 grad: 0.000576
Epoch: 150

 31%|█████████████████████████████████████                                                                                    | 153/500 [01:03<02:27,  2.36it/s]
Epoch: 151
LOSS: train: 0.004758 grad:0.000620   |   test: 0.004968 grad: 0.000572
Epoch: 152
LOSS: train: 0.004760 grad:0.000621   |   test: 0.004948 grad: 0.000571
Epoch: 153
LOSS: train: 0.004739 grad:0.000619   |   test: 0.004929 grad: 0.000569
Epoch: 154
LOSS: train: 0.004725 grad:0.000617   |   test: 0.004905 grad: 0.000567
Epoch: 155

 32%|██████████████████████████████████████▏                                                                                  | 158/500 [01:05<02:15,  2.53it/s]
Epoch: 156
LOSS: train: 0.004700 grad:0.000613   |   test: 0.004866 grad: 0.000563
Epoch: 157
LOSS: train: 0.004656 grad:0.000612   |   test: 0.004840 grad: 0.000561
Epoch: 158
LOSS: train: 0.004626 grad:0.000609   |   test: 0.004820 grad: 0.000559
Epoch: 159
LOSS: train: 0.004643 grad:0.000608   |   test: 0.004798 grad: 0.000558
Epoch: 160

 33%|███████████████████████████████████████▍                                                                                 | 163/500 [01:07<02:13,  2.52it/s]
Epoch: 161
LOSS: train: 0.004560 grad:0.000603   |   test: 0.004761 grad: 0.000555
Epoch: 162
LOSS: train: 0.004619 grad:0.000612   |   test: 0.004744 grad: 0.000553
Epoch: 163
LOSS: train: 0.004551 grad:0.000605   |   test: 0.004731 grad: 0.000551
Epoch: 164
LOSS: train: 0.004531 grad:0.000600   |   test: 0.004718 grad: 0.000550
Epoch: 165
 34%|████████████████████████████████████████▋                                                                                | 168/500 [01:09<02:18,  2.40it/s]
Traceback (most recent call last):
  File "/home/denis/Desktop/master/masterthesis15_03-13_09-2024/recap/corrected/mlp_func/main_mlp_vec.py", line 127, in <module>
    loss.backward()
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torch/autograd/function.py", line 267, in apply
    return user_fn(self, *args)
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torchdiffeq/_impl/adjoint.py", line 126, in backward
    aug_state = odeint(
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 105, in integrate
    dy, f0 = self._step_func(self.func, t0, dt, t1, y0)
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torchdiffeq/_impl/fixed_grid.py", line 29, in _step_func
    return rk4_alt_step_func(func, t0, dt, t1, y0, f0=f0, perturb=self.perturb), f0
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 112, in rk4_alt_step_func
    k3 = func(t0 + dt * _two_thirds, y0 + dt * (k2 - k1 * _one_third))
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 159, in forward
    return self.mul * self.base_func(-t, y)
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 138, in forward
    f = self.base_func(t, _flat_to_shape(y, (), self.shapes))
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torchdiffeq/_impl/adjoint.py", line 88, in augmented_dynamics
    func_eval = func(t if t_requires_grad else t_, y)
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/denis/Desktop/master/masterthesis15_03-13_09-2024/recap/corrected/mlp_func/mlp.py", line 42, in forward
    return self.net(y.float())
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/denis/anaconda3/envs/hnn/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Epoch: 166
LOSS: train: 0.004497 grad:0.000599   |   test: 0.004691 grad: 0.000547
Epoch: 167
LOSS: train: 0.004447 grad:0.000591   |   test: 0.004672 grad: 0.000545
Epoch: 168
LOSS: train: 0.004455 grad:0.000592   |   test: 0.004650 grad: 0.000543